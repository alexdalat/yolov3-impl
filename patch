commit 6aff0b37b68968253de9a35e9ae547256e72053e
Author: alexdalat <alexdalat@Alexs-MacBook-Air-10.local>
Date:   Wed Nov 30 11:33:19 2022 -0500

    initial push

diff --git a/.gitignore b/.gitignore
new file mode 100644
index 0000000..38efcbc
--- /dev/null
+++ b/.gitignore
@@ -0,0 +1,2 @@
+.DS_Store
+.ipynb_checkpoints/
\ No newline at end of file
diff --git a/coco.names b/coco.names
new file mode 100644
index 0000000..09043a5
--- /dev/null
+++ b/coco.names
@@ -0,0 +1,21 @@
+person
+bicycle
+car
+motorbike
+aeroplane
+bus
+train
+truck
+boat
+traffic light
+fire hydrant
+stop sign
+parking meter
+bench
+bird
+cat
+dog
+horse
+sheep
+cow
+umbrella
\ No newline at end of file
diff --git a/detect.py b/detect.py
new file mode 100644
index 0000000..fb57a26
--- /dev/null
+++ b/detect.py
@@ -0,0 +1,123 @@
+# %%
+import cv2
+import numpy as np
+
+cap = cv2.VideoCapture(0)
+
+# Initialize the parameters
+confThreshold = 0.5  #Confidence threshold
+nmsThreshold = 0.4   #Non-maximum suppression threshold
+
+scale = 4
+
+# Load names of classes
+classesFile = "coco.names"
+classes = None
+with open(classesFile, 'rt') as f:
+	classes = f.read().rstrip('\n').split('\n')
+ 
+# Give the configuration and weight files for the model and load the network using them.
+modelConfiguration = "yolov3-tiny.cfg"
+modelWeights = "yolov3-tiny.weights"
+ 
+net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)
+net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
+net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
+
+# %%
+# Get the names of the output layers
+def getOutputsNames(net):
+    # Get the names of all the layers in the network
+    layersNames = net.getLayerNames()
+    # Get the names of the output layers, i.e. the layers with unconnected outputs
+    return [layersNames[i - 1] for i in net.getUnconnectedOutLayers()]
+
+# %%
+# Draw the predicted bounding box
+def drawPred(frame, classId, conf, left, top, right, bottom):
+	# Draw a bounding box.
+	cv2.rectangle(frame, (left, top), (right, bottom), (255, 178, 50), 3)
+	 
+	label = '%.2f' % conf
+		 
+	# Get the label for the class name and its confidence
+	if classes:
+		assert(classId < len(classes))
+		label = '%s: %s' % (classes[classId], label)
+ 
+	#Display the label at the top of the bounding box
+	font = cv2.FONT_HERSHEY_SIMPLEX
+	labelSize, baseLine = cv2.getTextSize(label, font, 0.5, 1)
+	top = max(top, labelSize[1])
+	#cv2.putText(frame, label, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))
+	cv2.putText(frame, label, (left, top - round(1.5*labelSize[1])), font, 1, (255,255,0))
+
+# %%
+# Remove the bounding boxes with low confidence using non-maxima suppression
+def postprocess(frame, outs):
+	frameHeight = frame.shape[0]
+	frameWidth = frame.shape[1]
+ 
+	# Scan through all the bounding boxes output from the network and keep only the
+	# ones with high confidence scores. Assign the box's class label as the class with the highest score.
+	classIds = []
+	confidences = []
+	boxes = []
+	for out in outs:
+		for detection in out:
+			scores = detection[5:]
+			classId = np.argmax(scores)
+			confidence = scores[classId]
+			if confidence > confThreshold:
+				center_x = int(detection[0] * frameWidth)
+				center_y = int(detection[1] * frameHeight)
+				width = int(detection[2] * frameWidth)
+				height = int(detection[3] * frameHeight)
+				left = int(center_x - width / 2)
+				top = int(center_y - height / 2)
+				classIds.append(classId)
+				confidences.append(float(confidence))
+				boxes.append([left, top, width, height])
+ 
+	# Perform non maximum suppression to eliminate redundant overlapping boxes with
+	# lower confidences.
+	indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)
+	for i in indices:
+		box = boxes[i]
+		left = box[0]
+		top = box[1]
+		width = box[2]
+		height = box[3]
+		drawPred(frame, classIds[i], confidences[i], left, top, left + width, top + height)
+
+# %%
+while cap.isOpened():
+	flags, img = cap.read()
+	img = cv2.flip(img, 1) # if your camera reverses your image
+	img_resized = cv2.resize(img, (int(img.shape[1]*(4/3)), int(img.shape[1]*(3/4)))) # convert to 3/4
+	
+
+	# must be multiple of 32
+	big_img = cv2.resize(img_resized, (512,384))
+	small_img = cv2.resize(img_resized, (int(big_img.shape[1]/scale), int(big_img.shape[0]/scale)))
+
+	blob = cv2.dnn.blobFromImage(small_img, 1/255,(small_img.shape[1],small_img.shape[0]),[0,0,0],crop=False)
+	net.setInput(blob)
+
+	outs = net.forward(getOutputsNames(net))
+	postprocess(img, outs)
+
+	t, _ = net.getPerfProfile()
+	label = 'Inference time: %.2f ms' % (t * 1000.0 / cv2.getTickFrequency())
+	cv2.putText(img, label, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))
+
+
+	cv2.imshow("image", img)
+	k=cv2.waitKey(1) & 0XFF
+	if k== 27:
+		break
+
+cap.release()
+cv2.destroyAllWindows() 
+
+
diff --git a/yolo3-impl.ipynb b/yolo3-impl.ipynb
new file mode 100644
index 0000000..f4fbe70
--- /dev/null
+++ b/yolo3-impl.ipynb
@@ -0,0 +1,194 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": 1,
+   "id": "102be7b4",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import cv2\n",
+    "import numpy as np\n",
+    "\n",
+    "cap = cv2.VideoCapture(0)\n",
+    "\n",
+    "# Initialize the parameters\n",
+    "confThreshold = 0.5  #Confidence threshold\n",
+    "nmsThreshold = 0.4   #Non-maximum suppression threshold\n",
+    "\n",
+    "scale = 4\n",
+    "\n",
+    "# Load names of classes\n",
+    "classesFile = \"coco.names\"\n",
+    "classes = None\n",
+    "with open(classesFile, 'rt') as f:\n",
+    "\tclasses = f.read().rstrip('\\n').split('\\n')\n",
+    " \n",
+    "# Give the configuration and weight files for the model and load the network using them.\n",
+    "modelConfiguration = \"yolov3.cfg\"\n",
+    "modelWeights = \"yolov3.weights\"\n",
+    " \n",
+    "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
+    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
+    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "id": "b8ff7b7b",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Get the names of the output layers\n",
+    "def getOutputsNames(net):\n",
+    "    # Get the names of all the layers in the network\n",
+    "    layersNames = net.getLayerNames()\n",
+    "    # Get the names of the output layers, i.e. the layers with unconnected outputs\n",
+    "    return [layersNames[i - 1] for i in net.getUnconnectedOutLayers()]"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 3,
+   "id": "d6573683",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Draw the predicted bounding box\n",
+    "def drawPred(frame, classId, conf, left, top, right, bottom):\n",
+    "\t# Draw a bounding box.\n",
+    "\tcv2.rectangle(frame, (left, top), (right, bottom), (255, 178, 50), 3)\n",
+    "\t \n",
+    "\tlabel = '%.2f' % conf\n",
+    "\t\t \n",
+    "\t# Get the label for the class name and its confidence\n",
+    "\tif classes:\n",
+    "\t\tassert(classId < len(classes))\n",
+    "\t\tlabel = '%s: %s' % (classes[classId], label)\n",
+    " \n",
+    "\t#Display the label at the top of the bounding box\n",
+    "\tfont = cv2.FONT_HERSHEY_SIMPLEX\n",
+    "\tlabelSize, baseLine = cv2.getTextSize(label, font, 0.5, 1)\n",
+    "\ttop = max(top, labelSize[1])\n",
+    "\t#cv2.putText(frame, label, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))\n",
+    "\tcv2.putText(frame, label, (left, top - round(1.5*labelSize[1])), font, 1, (255,255,0))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 4,
+   "id": "00bab1df",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Remove the bounding boxes with low confidence using non-maxima suppression\n",
+    "def postprocess(frame, outs):\n",
+    "\tframeHeight = frame.shape[0]\n",
+    "\tframeWidth = frame.shape[1]\n",
+    " \n",
+    "\t# Scan through all the bounding boxes output from the network and keep only the\n",
+    "\t# ones with high confidence scores. Assign the box's class label as the class with the highest score.\n",
+    "\tclassIds = []\n",
+    "\tconfidences = []\n",
+    "\tboxes = []\n",
+    "\tfor out in outs:\n",
+    "\t\tfor detection in out:\n",
+    "\t\t\tscores = detection[5:]\n",
+    "\t\t\tclassId = np.argmax(scores)\n",
+    "\t\t\tconfidence = scores[classId]\n",
+    "\t\t\tif confidence > confThreshold:\n",
+    "\t\t\t\tcenter_x = int(detection[0] * frameWidth)\n",
+    "\t\t\t\tcenter_y = int(detection[1] * frameHeight)\n",
+    "\t\t\t\twidth = int(detection[2] * frameWidth)\n",
+    "\t\t\t\theight = int(detection[3] * frameHeight)\n",
+    "\t\t\t\tleft = int(center_x - width / 2)\n",
+    "\t\t\t\ttop = int(center_y - height / 2)\n",
+    "\t\t\t\tclassIds.append(classId)\n",
+    "\t\t\t\tconfidences.append(float(confidence))\n",
+    "\t\t\t\tboxes.append([left, top, width, height])\n",
+    " \n",
+    "\t# Perform non maximum suppression to eliminate redundant overlapping boxes with\n",
+    "\t# lower confidences.\n",
+    "\tindices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
+    "\tfor i in indices:\n",
+    "\t\tbox = boxes[i]\n",
+    "\t\tleft = box[0]\n",
+    "\t\ttop = box[1]\n",
+    "\t\twidth = box[2]\n",
+    "\t\theight = box[3]\n",
+    "\t\tdrawPred(frame, classIds[i], confidences[i], left, top, left + width, top + height)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
+   "id": "819c7c2e",
+   "metadata": {},
+   "outputs": [
+    {
+     "ename": "KeyboardInterrupt",
+     "evalue": "",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
+      "Cell \u001b[0;32mIn [5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwhile\u001b[39;00m cap\u001b[39m.\u001b[39misOpened():\n\u001b[0;32m----> 2\u001b[0m \tflags, img \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[1;32m      3\u001b[0m \timg \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mflip(img, \u001b[39m1\u001b[39m) \u001b[39m# if your camera reverses your image\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \timg_resized \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(img, (\u001b[39mint\u001b[39m(img\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m(\u001b[39m4\u001b[39m\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m)), \u001b[39mint\u001b[39m(img\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m(\u001b[39m3\u001b[39m\u001b[39m/\u001b[39m\u001b[39m4\u001b[39m)))) \u001b[39m# convert to 3/4\u001b[39;00m\n",
+      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
+     ]
+    }
+   ],
+   "source": [
+    "while cap.isOpened():\n",
+    "\tflags, img = cap.read()\n",
+    "\timg = cv2.flip(img, 1) # if your camera reverses your image\n",
+    "\timg_resized = cv2.resize(img, (int(img.shape[1]*(4/3)), int(img.shape[1]*(3/4)))) # convert to 3/4\n",
+    "\t\n",
+    "\n",
+    "\t# must be multiple of 32\n",
+    "\tbig_img = cv2.resize(img_resized, (512,384))\n",
+    "\tsmall_img = cv2.resize(img_resized, (int(big_img.shape[1]/scale), int(big_img.shape[0]/scale)))\n",
+    "\n",
+    "\tblob = cv2.dnn.blobFromImage(small_img, 1/255,(small_img.shape[1],small_img.shape[0]),[0,0,0],crop=False)\n",
+    "\tnet.setInput(blob)\n",
+    "\n",
+    "\touts = net.forward(getOutputsNames(net))\n",
+    "\tpostprocess(img, outs)\n",
+    "\n",
+    "\tt, _ = net.getPerfProfile()\n",
+    "\tlabel = 'Inference time: %.2f ms' % (t * 1000.0 / cv2.getTickFrequency())\n",
+    "\tcv2.putText(img, label, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))\n",
+    "\n",
+    "\n",
+    "\tcv2.imshow(\"image\", img)\n",
+    "\tk=cv2.waitKey(1) & 0XFF\n",
+    "\tif k== 27:\n",
+    "\t\tbreak\n",
+    "\n",
+    "cap.release()\n",
+    "cv2.destroyAllWindows() "
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "Python 3 (ipykernel)",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.10.6"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 5
+}
diff --git a/yolov3-tiny.cfg b/yolov3-tiny.cfg
new file mode 100644
index 0000000..f46a4f1
--- /dev/null
+++ b/yolov3-tiny.cfg
@@ -0,0 +1,182 @@
+[net]
+# Testing
+batch=1
+subdivisions=1
+# Training
+# batch=64
+# subdivisions=2
+width=416
+height=416
+channels=3
+momentum=0.9
+decay=0.0005
+angle=0
+saturation = 1.5
+exposure = 1.5
+hue=.1
+
+learning_rate=0.001
+burn_in=1000
+max_batches = 500200
+policy=steps
+steps=400000,450000
+scales=.1,.1
+
+[convolutional]
+batch_normalize=1
+filters=16
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[maxpool]
+size=2
+stride=2
+
+[convolutional]
+batch_normalize=1
+filters=32
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[maxpool]
+size=2
+stride=2
+
+[convolutional]
+batch_normalize=1
+filters=64
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[maxpool]
+size=2
+stride=2
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[maxpool]
+size=2
+stride=2
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[maxpool]
+size=2
+stride=2
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[maxpool]
+size=2
+stride=1
+
+[convolutional]
+batch_normalize=1
+filters=1024
+size=3
+stride=1
+pad=1
+activation=leaky
+
+###########
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+size=1
+stride=1
+pad=1
+filters=255
+activation=linear
+
+
+
+[yolo]
+mask = 3,4,5
+anchors = 10,14,  23,27,  37,58,  81,82,  135,169,  344,319
+classes=80
+num=6
+jitter=.3
+ignore_thresh = .7
+truth_thresh = 1
+random=1
+
+[route]
+layers = -4
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[upsample]
+stride=2
+
+[route]
+layers = -1, 8
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+size=1
+stride=1
+pad=1
+filters=255
+activation=linear
+
+[yolo]
+mask = 0,1,2
+anchors = 10,14,  23,27,  37,58,  81,82,  135,169,  344,319
+classes=80
+num=6
+jitter=.3
+ignore_thresh = .7
+truth_thresh = 1
+random=1
\ No newline at end of file
diff --git a/yolov3-tiny.weights b/yolov3-tiny.weights
new file mode 100644
index 0000000..aad7e6c
Binary files /dev/null and b/yolov3-tiny.weights differ
diff --git a/yolov3.cfg b/yolov3.cfg
new file mode 100644
index 0000000..c8bf7fc
--- /dev/null
+++ b/yolov3.cfg
@@ -0,0 +1,788 @@
+[net]
+# Testing
+# batch=1
+# subdivisions=1
+# Training
+batch=64
+subdivisions=16
+width=608
+height=608
+channels=3
+momentum=0.9
+decay=0.0005
+angle=0
+saturation = 1.5
+exposure = 1.5
+hue=.1
+
+learning_rate=0.001
+burn_in=1000
+max_batches = 500200
+policy=steps
+steps=400000,450000
+scales=.1,.1
+
+[convolutional]
+batch_normalize=1
+filters=32
+size=3
+stride=1
+pad=1
+activation=leaky
+
+# Downsample
+
+[convolutional]
+batch_normalize=1
+filters=64
+size=3
+stride=2
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=32
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=64
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+# Downsample
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=3
+stride=2
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=64
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+[convolutional]
+batch_normalize=1
+filters=64
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+# Downsample
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=3
+stride=2
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+# Downsample
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=3
+stride=2
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+# Downsample
+
+[convolutional]
+batch_normalize=1
+filters=1024
+size=3
+stride=2
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=1024
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=1024
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=1024
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=1024
+size=3
+stride=1
+pad=1
+activation=leaky
+
+[shortcut]
+from=-3
+activation=linear
+
+######################
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+size=3
+stride=1
+pad=1
+filters=1024
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+size=3
+stride=1
+pad=1
+filters=1024
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=512
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+size=3
+stride=1
+pad=1
+filters=1024
+activation=leaky
+
+[convolutional]
+size=1
+stride=1
+pad=1
+filters=255
+activation=linear
+
+
+[yolo]
+mask = 6,7,8
+anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
+classes=80
+num=9
+jitter=.3
+ignore_thresh = .7
+truth_thresh = 1
+random=1
+
+
+[route]
+layers = -4
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[upsample]
+stride=2
+
+[route]
+layers = -1, 61
+
+
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+size=3
+stride=1
+pad=1
+filters=512
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+size=3
+stride=1
+pad=1
+filters=512
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=256
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+size=3
+stride=1
+pad=1
+filters=512
+activation=leaky
+
+[convolutional]
+size=1
+stride=1
+pad=1
+filters=255
+activation=linear
+
+
+[yolo]
+mask = 3,4,5
+anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
+classes=80
+num=9
+jitter=.3
+ignore_thresh = .7
+truth_thresh = 1
+random=1
+
+
+
+[route]
+layers = -4
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[upsample]
+stride=2
+
+[route]
+layers = -1, 36
+
+
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+size=3
+stride=1
+pad=1
+filters=256
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+size=3
+stride=1
+pad=1
+filters=256
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+filters=128
+size=1
+stride=1
+pad=1
+activation=leaky
+
+[convolutional]
+batch_normalize=1
+size=3
+stride=1
+pad=1
+filters=256
+activation=leaky
+
+[convolutional]
+size=1
+stride=1
+pad=1
+filters=255
+activation=linear
+
+
+[yolo]
+mask = 0,1,2
+anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
+classes=80
+num=9
+jitter=.3
+ignore_thresh = .7
+truth_thresh = 1
+random=1
diff --git a/yolov3.weights b/yolov3.weights
new file mode 100644
index 0000000..a5ed716
Binary files /dev/null and b/yolov3.weights differ
